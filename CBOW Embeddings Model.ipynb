{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import os\n",
    "\n",
    "def parse_data(file):\n",
    "    for l in open(file, 'r'):\n",
    "        yield json.loads(l)\n",
    "        \n",
    "data = pd.DataFrame(parse_data('..\\data\\\\Sarcasm_Headlines_Dataset_v2.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sarcastic</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
       "      <td>https://www.theonion.com/thirtysomething-scien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>dem rep. totally nails why congress is falling...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/donna-edw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/eat-your-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>inclement weather prevents liar from getting t...</td>\n",
       "      <td>https://local.theonion.com/inclement-weather-p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>mother comes pretty close to using word 'strea...</td>\n",
       "      <td>https://www.theonion.com/mother-comes-pretty-c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sarcastic                                           Headline  \\\n",
       "0          1  thirtysomething scientists unveil doomsday clo...   \n",
       "1          0  dem rep. totally nails why congress is falling...   \n",
       "2          0  eat your veggies: 9 deliciously different recipes   \n",
       "3          1  inclement weather prevents liar from getting t...   \n",
       "4          1  mother comes pretty close to using word 'strea...   \n",
       "\n",
       "                                                Link  \n",
       "0  https://www.theonion.com/thirtysomething-scien...  \n",
       "1  https://www.huffingtonpost.com/entry/donna-edw...  \n",
       "2  https://www.huffingtonpost.com/entry/eat-your-...  \n",
       "3  https://local.theonion.com/inclement-weather-p...  \n",
       "4  https://www.theonion.com/mother-comes-pretty-c...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns = ['Sarcastic', 'Headline', 'Link']\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Link'] = data['Link'].apply(lambda x: x.split('.')[1].capitalize()) #extracts publications\n",
    "data.rename(columns = {'Link': 'Publication'}, inplace = True) #This is not immediately useful, but I'm doing this just in case we use this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_removed = []\n",
    "for i in data['Headline']:\n",
    "    i = re.sub(r\"[^a-zA-Z\\s]\", \"\", i)\n",
    "    main_words = ' '.join([j for j in i.split() if j.lower() not in ENGLISH_STOP_WORDS])\n",
    "    stop_words_removed.append(main_words)\n",
    "\n",
    "tokenized_corpus = [word_tokenize(i) for i in stop_words_removed]\n",
    "vocab = set([word for sentence in tokenized_corpus for word in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, embedding_size, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.linear = nn.Linear(embedding_size, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embeddings(inputs).mean(1)\n",
    "        return self.linear(embeddings)\n",
    "    \n",
    " \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "idx_to_word = {i: word for i, word in enumerate(vocab)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_data = []\n",
    "for i in tokenized_corpus:\n",
    "    for j in range(2, len(i)-2):\n",
    "        context = [\n",
    "            i[j-2],\n",
    "            i[j-1],\n",
    "            i[j+1],\n",
    "            i[j+2]]\n",
    "        target = i[j]\n",
    "        \n",
    "        context_idxs = [word_to_idx[k] for k in context]\n",
    "        target_idx = word_to_idx[target]\n",
    "        context_data.append((context_idxs, target_idx))\n",
    "\n",
    "#We will use a 60 - 20 - 20 split, defined in training function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingCBOW():\n",
    "    embedding_size = 50\n",
    "    batch_size = 32\n",
    "    learning_rate = 0.001\n",
    "    epochs = 50\n",
    "\n",
    "    print(f\"Training CBOW with Embedding Size: {embedding_size}, LR: {learning_rate}\")\n",
    "\n",
    "    vocab_size = len(word_to_idx)\n",
    "    net = CBOW(embedding_size=embedding_size, vocab_size=vocab_size)\n",
    "    net.train()\n",
    "\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    context, target = zip(*context_data)\n",
    "    context_tensor = torch.tensor(context, dtype=torch.long)\n",
    "    target_tensor = torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "    final_dataset = TensorDataset(context_tensor, target_tensor)\n",
    "\n",
    "    dataset_size = len(final_dataset)\n",
    "    train_size = int(0.6 * dataset_size)\n",
    "    test_size = dataset_size - train_size\n",
    "\n",
    "    train_dataset, test_dataset = random_split(final_dataset, [train_size, test_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        net.train()\n",
    "\n",
    "        for con, tar in train_loader:\n",
    "            optimizer.zero_grad()  \n",
    "            output = net(con)  \n",
    "            loss = loss_function(output, tar)  \n",
    "            loss.backward()  \n",
    "            optimizer.step()  \n",
    "\n",
    "            total_loss += loss.item()  \n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {total_loss:.4f}\")\n",
    "\n",
    "    torch.save(net.state_dict(), os.path.join(os.getcwd(), \"cbow_model.pth\"))\n",
    "    print(\"Model saved!\")\n",
    "\n",
    "    return net\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CBOW with Embedding Size: 50, LR: 0.001\n",
      "Epoch 1/50 | Train Loss: 15084.3544\n",
      "Epoch 2/50 | Train Loss: 13359.2523\n",
      "Epoch 3/50 | Train Loss: 12498.0950\n",
      "Epoch 4/50 | Train Loss: 11825.5349\n",
      "Epoch 5/50 | Train Loss: 11191.7801\n",
      "Epoch 6/50 | Train Loss: 10573.1579\n",
      "Epoch 7/50 | Train Loss: 9964.2801\n",
      "Epoch 8/50 | Train Loss: 9366.3468\n",
      "Epoch 9/50 | Train Loss: 8781.3365\n",
      "Epoch 10/50 | Train Loss: 8211.8560\n",
      "Epoch 11/50 | Train Loss: 7659.3167\n",
      "Epoch 12/50 | Train Loss: 7126.3360\n",
      "Epoch 13/50 | Train Loss: 6615.4802\n",
      "Epoch 14/50 | Train Loss: 6130.5224\n",
      "Epoch 15/50 | Train Loss: 5673.4378\n",
      "Epoch 16/50 | Train Loss: 5248.2726\n",
      "Epoch 17/50 | Train Loss: 4853.9102\n",
      "Epoch 18/50 | Train Loss: 4493.1320\n",
      "Epoch 19/50 | Train Loss: 4160.8545\n",
      "Epoch 20/50 | Train Loss: 3857.3161\n",
      "Epoch 21/50 | Train Loss: 3578.2639\n",
      "Epoch 22/50 | Train Loss: 3319.7355\n",
      "Epoch 23/50 | Train Loss: 3081.4636\n",
      "Epoch 24/50 | Train Loss: 2859.6389\n",
      "Epoch 25/50 | Train Loss: 2653.4197\n",
      "Epoch 26/50 | Train Loss: 2460.4354\n",
      "Epoch 27/50 | Train Loss: 2280.3698\n",
      "Epoch 28/50 | Train Loss: 2112.8368\n",
      "Epoch 29/50 | Train Loss: 1954.3370\n",
      "Epoch 30/50 | Train Loss: 1807.7612\n",
      "Epoch 31/50 | Train Loss: 1668.2670\n",
      "Epoch 32/50 | Train Loss: 1538.0492\n",
      "Epoch 33/50 | Train Loss: 1415.6551\n",
      "Epoch 34/50 | Train Loss: 1302.3506\n",
      "Epoch 35/50 | Train Loss: 1195.0092\n",
      "Epoch 36/50 | Train Loss: 1095.7496\n",
      "Epoch 37/50 | Train Loss: 1002.6007\n",
      "Epoch 38/50 | Train Loss: 914.8269\n",
      "Epoch 39/50 | Train Loss: 834.5708\n",
      "Epoch 40/50 | Train Loss: 758.1062\n",
      "Epoch 41/50 | Train Loss: 688.1020\n",
      "Epoch 42/50 | Train Loss: 622.8997\n",
      "Epoch 43/50 | Train Loss: 561.9499\n",
      "Epoch 44/50 | Train Loss: 505.6802\n",
      "Epoch 45/50 | Train Loss: 454.1133\n",
      "Epoch 46/50 | Train Loss: 406.3505\n",
      "Epoch 47/50 | Train Loss: 362.5581\n",
      "Epoch 48/50 | Train Loss: 322.6200\n",
      "Epoch 49/50 | Train Loss: 285.7472\n",
      "Epoch 50/50 | Train Loss: 252.4156\n",
      "Model saved!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CBOW(\n",
       "  (embeddings): Embedding(28514, 50)\n",
       "  (linear): Linear(in_features=50, out_features=28514, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingCBOW()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\david\\\\OneDrive\\\\Desktop\\\\Python Projects\\\\134'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
