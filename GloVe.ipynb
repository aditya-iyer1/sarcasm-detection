{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk import word_tokenize\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "from torch.nn import Embedding\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(file):\n",
    "    for l in open(file, 'r'):\n",
    "        yield json.loads(l)\n",
    "        \n",
    "data = pd.DataFrame(parse_data('C:\\\\Users\\\\david\\\\sarcasm-detection\\\\data\\\\Sarcasm_Headlines_Dataset_v2.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sarcastic</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
       "      <td>https://www.theonion.com/thirtysomething-scien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>dem rep. totally nails why congress is falling...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/donna-edw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/eat-your-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>inclement weather prevents liar from getting t...</td>\n",
       "      <td>https://local.theonion.com/inclement-weather-p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>mother comes pretty close to using word 'strea...</td>\n",
       "      <td>https://www.theonion.com/mother-comes-pretty-c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sarcastic                                           Headline  \\\n",
       "0          1  thirtysomething scientists unveil doomsday clo...   \n",
       "1          0  dem rep. totally nails why congress is falling...   \n",
       "2          0  eat your veggies: 9 deliciously different recipes   \n",
       "3          1  inclement weather prevents liar from getting t...   \n",
       "4          1  mother comes pretty close to using word 'strea...   \n",
       "\n",
       "                                                Link  \n",
       "0  https://www.theonion.com/thirtysomething-scien...  \n",
       "1  https://www.huffingtonpost.com/entry/donna-edw...  \n",
       "2  https://www.huffingtonpost.com/entry/eat-your-...  \n",
       "3  https://local.theonion.com/inclement-weather-p...  \n",
       "4  https://www.theonion.com/mother-comes-pretty-c...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns = ['Sarcastic', 'Headline', 'Link']\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_removed = []\n",
    "for i in data['Headline']:\n",
    "    i = re.sub(r\"[^a-zA-Z\\s]\", \"\", i)\n",
    "    main_words = ' '.join([j for j in i.split() if j.lower() not in ENGLISH_STOP_WORDS])\n",
    "    stop_words_removed.append(main_words)\n",
    "    \n",
    "tokenized_corpus = [word_tokenize(i) for i in stop_words_removed]\n",
    "vocab = set([word for sentence in tokenized_corpus for word in sentence])\n",
    "word_to_idx = {word: i for i, word in enumerate(vocab)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttsplit = 0.2 #Personal preference\n",
    "max_length = 25 #Number previously derived from tuning\n",
    "\n",
    "tokenized_corpus_num = []\n",
    "for headline in tokenized_corpus:\n",
    "    idx_seq = []\n",
    "    for word in headline:\n",
    "        idx_seq.append(word_to_idx[word])\n",
    "    tokenized_corpus_num.append(idx_seq)\n",
    "\n",
    "tensor_corpus = [torch.tensor(seq, dtype = torch.long) for seq in tokenized_corpus_num]\n",
    "padded_sequences = pad_sequence(tensor_corpus, batch_first=True,padding_value=len(vocab))\n",
    "padded_sequences = padded_sequences[:, :max_length]\n",
    "\n",
    "sarcasm = torch.tensor(data['Sarcastic'].values, dtype = torch.long)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, sarcasm, test_size = ttsplit,\n",
    "                                                    stratify = sarcasm, shuffle = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "embedding_dim = 100\n",
    "\n",
    "GloVe = open(os.path.join(os.getcwd(), 'glove.twitter.27B.100d.txt'), encoding = \"utf-8\")\n",
    "\n",
    "for entry in GloVe:\n",
    "    values = entry.split()\n",
    "    word = values[0]\n",
    "    coeffs = np.asarray(values[1:], dtype = 'float32')\n",
    "    embeddings_index[word] = coeffs\n",
    "GloVe.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_to_idx) + 1, embedding_dim))\n",
    "c = 0\n",
    "for word, i in word_to_idx.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        c+=1\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_to_idx) + 1\n",
    "class SarcasmLSTM(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim = 64, dropout = 0.2, rdropout = 0.25):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding_weights = torch.tensor(embedding_matrix, dtype = torch.float32)\n",
    "        self.embedding_layer = Embedding.from_pretrained(self.embedding_weights, freeze = True, padding_idx=len(vocab))\n",
    "\n",
    "    \n",
    "        self.lstm = nn.LSTM(input_size = 100, hidden_size = hidden_dim,\n",
    "                            batch_first=True, dropout=rdropout,\n",
    "                            bidirectional = True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim*2, 1)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding_layer(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "        \n",
    "class SarcasmDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "    \n",
    "train_dataset = SarcasmDataset(X_train, y_train)\n",
    "test_dataset = SarcasmDataset(X_test, y_test)\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30, Loss: 0.5552, Accuracy: 0.7110286088665647]\n",
      "Epoch [2/30, Loss: 0.4622, Accuracy: 0.7829220353789037]\n",
      "Epoch [3/30, Loss: 0.4202, Accuracy: 0.8059838392662153]\n",
      "Epoch [4/30, Loss: 0.3791, Accuracy: 0.828958287835772]\n",
      "Epoch [5/30, Loss: 0.3474, Accuracy: 0.845555798209216]\n",
      "Epoch [6/30, Loss: 0.3165, Accuracy: 0.8628084734658222]\n",
      "Epoch [7/30, Loss: 0.2877, Accuracy: 0.8773531338720244]\n",
      "Epoch [8/30, Loss: 0.2619, Accuracy: 0.8909368857829221]\n",
      "Epoch [9/30, Loss: 0.2367, Accuracy: 0.9056562568246342]\n",
      "Epoch [10/30, Loss: 0.2094, Accuracy: 0.9190216204411443]\n",
      "Epoch [11/30, Loss: 0.1881, Accuracy: 0.9257916575671544]\n",
      "Epoch [12/30, Loss: 0.1700, Accuracy: 0.936580039309893]\n",
      "Epoch [13/30, Loss: 0.1459, Accuracy: 0.9464075125573269]\n",
      "Epoch [14/30, Loss: 0.1328, Accuracy: 0.9534396156366018]\n",
      "Epoch [15/30, Loss: 0.1193, Accuracy: 0.9592050666084297]\n",
      "Epoch [16/30, Loss: 0.1061, Accuracy: 0.9647521292858703]\n",
      "Epoch [17/30, Loss: 0.0949, Accuracy: 0.9695566717623936]\n",
      "Epoch [18/30, Loss: 0.0905, Accuracy: 0.9716531993885128]\n",
      "Epoch [19/30, Loss: 0.0780, Accuracy: 0.9767634854771784]\n",
      "Epoch [20/30, Loss: 0.0739, Accuracy: 0.9778117492902381]\n",
      "Epoch [21/30, Loss: 0.0692, Accuracy: 0.9799519545752348]\n",
      "Epoch [22/30, Loss: 0.0702, Accuracy: 0.9786416248089103]\n",
      "Epoch [23/30, Loss: 0.0591, Accuracy: 0.9830530683555362]\n",
      "Epoch [24/30, Loss: 0.0597, Accuracy: 0.9826162917667614]\n",
      "Epoch [25/30, Loss: 0.0579, Accuracy: 0.9843633981218607]\n",
      "Epoch [26/30, Loss: 0.0494, Accuracy: 0.9872461236077746]\n",
      "Epoch [27/30, Loss: 0.0533, Accuracy: 0.9848001747106355]\n",
      "Epoch [28/30, Loss: 0.0517, Accuracy: 0.9863288927713475]\n",
      "Epoch [29/30, Loss: 0.0469, Accuracy: 0.9874208342432845]\n",
      "Epoch [30/30, Loss: 0.0473, Accuracy: 0.9867219917012449]\n",
      "Model Saved!\n"
     ]
    }
   ],
   "source": [
    "model2 = SarcasmLSTM(embedding_matrix = embedding_matrix)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model2.parameters(), lr = 0.001)\n",
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model2.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model2(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        predictions = (outputs > 0.5).float()\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "            \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy}]\")\n",
    "    \n",
    "torch.save(model2.state_dict, \"sarcasm_lstm_GloVe2.pth\")\n",
    "print('Model Saved!')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
